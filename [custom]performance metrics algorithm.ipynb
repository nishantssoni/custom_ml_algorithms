{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ab994b-8717-4baa-9a05-3fd213e52ae6",
   "metadata": {},
   "source": [
    "# Performance Metrics Implementation\r\n",
    "\r\n",
    "1. **Accuracy Score**\r\n",
    "   - Measures the ratio of correctly predicted instances out of the total instances.\r\n",
    "\r\n",
    "2. **Precision**\r\n",
    "   - Indicates the ratio of correctly predicted positive observations to the total predicted positives.\r\n",
    "\r\n",
    "3. **Recall (Sensitivity or True Positive Rate - TPR)**\r\n",
    "   - Represents the ratio of correctly predicted positive observations to all actual positives.\r\n",
    "\r\n",
    "4. **False Positive Rate (FPR)**\r\n",
    "   - Indicates the ratio of incorrectly predicted positive observations to all actual negatives.\r\n",
    "\r\n",
    "5. **F1 Score**\r\n",
    "   - The harmonic mean of precision and recall, providing a balance between the two.\r\n",
    "\r\n",
    "6. **ROC (Receiver Operating Characteristic) Curve**\r\n",
    "   - A graphical representation showing the trade-off between the TPR and FPR at various threshold settings.\r\n",
    "\r\n",
    "7. **ROC Plot**\r\n",
    "   - The visual plot of the ROC curve to illustrate the performance of the classification model.\r\n",
    "\r\n",
    "8. **AUC-ROC (Area Under the ROC Curve)**\r\n",
    "   - Quantifies the overall performance of the model by calculating the area under the ROC curve.\r\n",
    "\r\n",
    "9. **Optimal Threshold**\r\n",
    "   - The threshold value that provides the best trade-off between TPR and FPR, maximizing the model's performance.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b482e928-40df-4917-a9a2-7546bfdbbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34fe8bb-cf70-4af9-bd4d-b052f002017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Performace_matrix(df,thres=0.5):\n",
    "    \"\"\"return   [[TN FN]  a numpy array\n",
    "                [FP TP]]   \n",
    "                \n",
    "                the probablity score column should be name \n",
    "                proba it return a numpy array\"\"\"\n",
    "    \n",
    "    TN = TP = FN = FP = 0\n",
    "    \n",
    "    # comparing score and adding new column\n",
    "    df['y_hat'] = np.where(df['proba']<thres, 0.0, 1.0)\n",
    "    \n",
    "    #TN\n",
    "    result = ((df['y'] == 0) & (df['y_hat'] == 0)).values\n",
    "    for i in result:\n",
    "        if i == True:\n",
    "            TN += 1\n",
    "        \n",
    "    #FN\n",
    "    result = ((df['y'] == 1) & (df['y_hat'] == 0)).values\n",
    "    for i in result:\n",
    "        if i == True:\n",
    "            FN += 1\n",
    "\n",
    "    #FP\n",
    "    result = ((df['y'] == 0) & (df['y_hat'] == 1)).values\n",
    "    \n",
    "    for i in result:\n",
    "        if i == True:\n",
    "            FP += 1\n",
    "\n",
    "    #TP\n",
    "    result = ((df['y'] == 1) & (df['y_hat'] == 1)).values\n",
    "    \n",
    "    for i in result:\n",
    "        if i == True:\n",
    "            TP += 1\n",
    "    \n",
    "    return np.array([[TN,FN],[FP,TP]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db05666a-551d-4fcc-9136-b7acfa93e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TPR_and_FPR(confu):\n",
    "    TN = confu[0][0]\n",
    "    FN = confu[0][1]\n",
    "    FP = confu[1][0]\n",
    "    TP = confu[1][1]\n",
    "    \n",
    "    TPR = (TP / (TP+FN) )\n",
    "    FPR = ( FP / (TN + FP) )\n",
    "    return (TPR, FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dff2e30-8f4a-4cb2-a45e-8e232af5c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision and recall\n",
    "def cal_pre_rec(confu):\n",
    "    FN = confu[0][1]\n",
    "    FP = confu[1][0]\n",
    "    TP = confu[1][1]\n",
    "    \n",
    "    pre = (TP/(TP+FP))\n",
    "    rec = (TP/(TP+FN))\n",
    "      \n",
    "    return (pre, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31363d4-631a-4538-8c65-c2b761cb0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute f1 score\n",
    "def cal_F1_score(confu):\n",
    "    \n",
    "    pre, rec = cal_pre_rec(confu)\n",
    "    \n",
    "    score = ( (2*pre*rec) / (pre + rec) )\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f88a297-2078-4968-8348-c8483234d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy score\n",
    "def cal_accuracy_score(confu):\n",
    "    TN = confu[0][0]\n",
    "    FN = confu[0][1]\n",
    "    FP = confu[1][0]\n",
    "    TP = confu[1][1]\n",
    "    \n",
    "    return ((TN+TP) / (TN+TP+FN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd380b6-38aa-48dc-a11a-59a58cefbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc curve\n",
    "def ROC(df):\n",
    "    # sort the value in place\n",
    "    sorted_df = df.sort_values(by='proba', ascending=False)\n",
    "    \n",
    "    _thers = list()\n",
    "    _tpr = list()\n",
    "    _fpr = list()\n",
    "    \n",
    "    for i in tqdm(sorted_df['proba']):\n",
    "        confu = compute_Performace_matrix(df=sorted_df, thres=i)\n",
    "        tpr, fpr = compute_TPR_and_FPR(confu)\n",
    "        _thers.append(i)\n",
    "        _tpr.append(tpr)\n",
    "        _fpr.append(fpr)\n",
    "\n",
    "    return np.array(_thers), np.array(_tpr), np.array(_fpr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fd91d9-dea9-4471-ad5d-d06a1da894b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curv(thresholds, tpr, fpr):\n",
    "    # Generate a trace for ROC curve\n",
    "    trace0 = go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines',\n",
    "        name='ROC curve'\n",
    "    )\n",
    "\n",
    "    # Only label every nth point to avoid cluttering\n",
    "    n = 1000  \n",
    "    indices = np.arange(len(thresholds)) % n == 0  # Choose indices where index mod n is 0\n",
    "\n",
    "    trace1 = go.Scatter(\n",
    "        x=fpr[indices], \n",
    "        y=tpr[indices], \n",
    "        mode='markers+text', \n",
    "        name='Threshold points', \n",
    "        text=[f\"Thr={thr:.2f}\" for thr in thresholds[indices]], \n",
    "        textposition='top center'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Diagonal line\n",
    "    trace2 = go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1], \n",
    "        mode='lines', \n",
    "        name='Random (Area = 0.5)', \n",
    "        line=dict(dash='dash')\n",
    "    )\n",
    "\n",
    "    data = [trace0, trace1, trace2]\n",
    "\n",
    "    # Define layout with square aspect ratio\n",
    "    layout = go.Layout(\n",
    "        title='Receiver Operating Characteristic',\n",
    "        xaxis=dict(title='False Positive Rate'),\n",
    "        yaxis=dict(title='True Positive Rate'),\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Define figure and add data\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e93e28-20bb-45d8-bd1f-c4710b9d2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under curv of ROC\n",
    "def auc_roc(tpr, fpr):\n",
    "    return np.trapz(tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d74b78-63e5-475b-b58b-6f40e9a018af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_threshold(tpr, fpr):\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cd6d4-400d-42cb-9ea7-475a8f8645c2",
   "metadata": {},
   "source": [
    "# implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2b4fe-cae6-4920-9f12-27d811a03ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
